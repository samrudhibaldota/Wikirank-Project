# -*- coding: utf-8 -*-
"""pagerank_sample.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1o6Enn4uwYdqEK-HU4cZ8fNC3quMWMVkR
"""

from pyspark.sql import SparkSession
from pyspark.sql.functions import col
import json

spark = SparkSession.builder \
    .appName("WikipediaPageRankSample") \
    .config("spark.jars", "/Users/samrudhibaldota/jars/graphframes-0.8.1-spark3.0-s_2.12.jar") \
    .getOrCreate()

spark.sparkContext.setLogLevel("ERROR")

# 1) Load the sample
rdd = spark.sparkContext.textFile("wiki_parsed_sample.json")
parsed = rdd.map(lambda line: json.loads(line))

# 2) Build edges RDD/DataFrame
edges = parsed.flatMap(
    lambda row: [(row['title'], link) for link in row['links'] if row['title'] != link]
)
edges_df = edges.toDF(["src", "dst"])

# 3) Build vertices DataFrame
vertices_df = (
    edges_df.select("src")
    .union(edges_df.select("dst"))
    .distinct()
    .withColumnRenamed("src", "id")
)

# 4) Import and build GraphFrame
from graphframes import GraphFrame
g = GraphFrame(vertices_df, edges_df)

# 5) Run PageRank (10 iterations)
results = g.pageRank(resetProbability=0.15, maxIter=10)

# 6) Show top 10
results.vertices \
    .select("id", "pagerank") \
    .orderBy(col("pagerank").desc()) \
    .show(10, truncate=False)

# 7) (Optional) Save full sample results
results.vertices \
    .orderBy(col("pagerank").desc()) \
    .write \
    .json("pagerank_sample_results")