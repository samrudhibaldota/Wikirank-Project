# -*- coding: utf-8 -*-
"""extract_wiki.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1dn9Jx-qyy85cHxjDa9xp3dZC9G87Y1zF
"""

import bz2
import re
from lxml import etree
import json

NAMESPACE = '{http://www.mediawiki.org/xml/export-0.11/}'

def extract_pages(input_path, output_path):
    count = 0
    with bz2.open(input_path, 'rb') as f_in, open(output_path, 'w', encoding='utf-8') as f_out:
        context = etree.iterparse(f_in, events=('end',), tag=NAMESPACE + 'page')
        for event, elem in context:
            title_elem = elem.find(NAMESPACE + 'title')
            rev_elem = elem.find(NAMESPACE + 'revision')
            text_elem = rev_elem.find(NAMESPACE + 'text') if rev_elem is not None else None

            title = title_elem.text if title_elem is not None else ''
            text = text_elem.text if text_elem is not None else ''
            links = re.findall(r'\[\[(.*?)\]\]', text or '')
            links = [l.split('|')[0].strip() for l in links if l.strip()]

            json.dump({'title': title, 'text': text, 'links': links}, f_out)
            f_out.write('\n')

            count += 1
            if count % 1000 == 0:
                print(f"âœ… Processed {count} articles...")

            elem.clear()

    print(f"\nðŸŽ‰ Done! Total articles extracted: {count}")

# ðŸ”½ This is what actually runs the function
if __name__ == "__main__":
    extract_pages(
        input_path="simplewiki-latest-pages-articles.xml.bz2",
        output_path="wiki_parsed.json"
    )